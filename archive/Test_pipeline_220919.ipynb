{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc336c0-5ea2-459d-92c5-444d766d066e",
   "metadata": {},
   "source": [
    "#### 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9deb58c4-799e-4c90-a936-3aa2e77d4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run installs \n",
    "# %pip install torch\n",
    "# %pip install git+https://github.com/facebookresearch/fairscale\n",
    "# %pip install git+https://github.com/facebookresearch/fvcore\n",
    "# %pip install -U iopath \n",
    "# %pip install simplejson\n",
    "# %pip install psutil\n",
    "# %pip install torchsummary\n",
    "# !python setup.py build develop\n",
    "# !pip install torchmetrics\n",
    "#%pip install helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2057e-ac15-426e-8a16-d965e4e97f73",
   "metadata": {},
   "source": [
    "#### 2. Set up environment, model registry, data registry and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c636913e-4849-493a-b7c5-bec12c13c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Programs\\anaconda\\envs\\hyka_ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import datetime\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report , precision_recall_fscore_support\n",
    "import iopath\n",
    "import simplejson\n",
    "import psutil\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b78fa2-bb5b-4697-894a-d9eedcde45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:\\OneDrive - The University of Sydney (Students)\\DATA5703 Capstone Project (Group)\\Scripts\\usyd_cs10_2\\MVIT\n",
      " Volume in drive H is Internal Storage\n",
      " Volume Serial Number is 288B-93B9\n",
      "\n",
      " Directory of h:\\OneDrive - The University of Sydney (Students)\\DATA5703 Capstone Project (Group)\\Scripts\\usyd_cs10_2\\MVIT\n",
      "\n",
      "18/09/2022  05:46 PM    <DIR>          .\n",
      "18/09/2022  05:46 PM    <DIR>          ..\n",
      "17/09/2022  11:02 PM               480 .flake8\n",
      "21/09/2022  01:32 AM    <DIR>          archive\n",
      "18/09/2022  05:37 PM        20,540,149 archive.zip\n",
      "17/09/2022  11:02 PM             3,615 CODE_OF_CONDUCT.md\n",
      "18/09/2022  04:28 PM    <DIR>          configs\n",
      "17/09/2022  11:02 PM             1,472 CONTRIBUTING.md\n",
      "17/09/2022  11:02 PM             1,116 INSTALL.md\n",
      "17/09/2022  11:02 PM            10,443 LICENSE\n",
      "17/09/2022  11:02 PM               680 linter.sh\n",
      "18/09/2022  04:28 PM    <DIR>          mvit\n",
      "17/09/2022  11:18 PM       290,305,429 MViTv2_T_in1k.pyth\n",
      "17/09/2022  11:02 PM             3,542 README.md\n",
      "17/09/2022  11:02 PM               753 setup.cfg\n",
      "17/09/2022  11:02 PM               573 setup.py\n",
      "18/09/2022  04:28 PM    <DIR>          tools\n",
      "              11 File(s)    310,868,252 bytes\n",
      "               6 Dir(s)  341,494,673,408 bytes free\n"
     ]
    }
   ],
   "source": [
    "# make sure you're in the main folder\n",
    "%cd ./MVIT\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d8a860-588b-4257-8dd4-8df0ecd2a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvit.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0ff113-29f7-49e4-a287-d1164a1b659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fvcore.common.registry import Registry\n",
    "\n",
    "MODEL_REGISTRY = Registry(\"MODEL\")\n",
    "MODEL_REGISTRY.__doc__ = \"\"\"\n",
    "Registry for models.\n",
    "The registered object will be called with `obj(cfg)`.\n",
    "The call should return a `torch.nn.Module` object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e2b1b5-838b-4e67-84a7-ac08b97c2b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Registry of MODEL:\n",
       "╒═════════╤═══════════╕\n",
       "│ Names   │ Objects   │\n",
       "╞═════════╪═══════════╡\n",
       "╘═════════╧═══════════╛"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can see the model registry is empty \n",
    "MODEL_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c3cb39-adda-4e0d-8199-c022e8b98419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste this code from the build.py file and run separately\n",
    "def build_model(cfg, gpu_id=None):\n",
    "    \"\"\"\n",
    "    Builds the model.\n",
    "    Args:\n",
    "        cfg (configs): configs that contains the hyper-parameters to build the\n",
    "        backbone. Details can be seen in mvit/config/defaults.py.\n",
    "        gpu_id (Optional[int]): specify the gpu index to build model.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        assert (\n",
    "            cfg.NUM_GPUS <= torch.cuda.device_count()\n",
    "        ), \"Cannot use more GPU devices than available\"\n",
    "    else:\n",
    "        assert (\n",
    "            cfg.NUM_GPUS == 0\n",
    "        ), \"Cuda is not available. Please set `NUM_GPUS: 0 for running on CPUs.\"\n",
    "\n",
    "    # Construct the model\n",
    "    name = cfg.MODEL.MODEL_NAME\n",
    "    model = MODEL_REGISTRY.get(name)(cfg)\n",
    "\n",
    "    if cfg.NUM_GPUS:\n",
    "        if gpu_id is None:\n",
    "            # Determine the GPU used by the current process\n",
    "            cur_device = torch.cuda.current_device()\n",
    "        else:\n",
    "            cur_device = gpu_id\n",
    "        # Transfer the model to the current GPU device\n",
    "        model = model.cuda(device=cur_device)\n",
    "    # Use multi-process data parallel model in the multi-gpu setting\n",
    "    if cfg.NUM_GPUS > 1:\n",
    "        # Make model replica operate on the current device\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            module=model, device_ids=[cur_device], output_device=cur_device\n",
    "        )\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce580c40-4278-4baf-8f87-51b1b13bd9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:\\OneDrive - The University of Sydney (Students)\\DATA5703 Capstone Project (Group)\\Scripts\\usyd_cs10_2\\MVIT\\tools\n",
      " Volume in drive H is Internal Storage\n",
      " Volume Serial Number is 288B-93B9\n",
      "\n",
      " Directory of h:\\OneDrive - The University of Sydney (Students)\\DATA5703 Capstone Project (Group)\\Scripts\\usyd_cs10_2\\MVIT\\tools\n",
      "\n",
      "18/09/2022  04:28 PM    <DIR>          .\n",
      "18/09/2022  04:28 PM    <DIR>          ..\n",
      "18/09/2022  04:28 PM    <DIR>          __pycache__\n",
      "17/09/2022  11:02 PM            11,479 engine.py\n",
      "17/09/2022  11:02 PM             3,593 main.py\n",
      "               2 File(s)         15,072 bytes\n",
      "               3 Dir(s)  341,494,673,408 bytes free\n"
     ]
    }
   ],
   "source": [
    "%cd ./tools\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eaf1fd1-3151-46bc-bcaf-2fdfd034c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the get_cfg command separately - this is needed for the build model step. This is in the engine.py file \n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import mvit.utils.checkpoint as cu\n",
    "from engine import test, train\n",
    "from mvit.config.defaults import assert_and_infer_cfg, get_cfg\n",
    "from mvit.utils.misc import launch_job\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse the following arguments for a default parser.\n",
    "    Args:\n",
    "        shard_id (int): shard id for the current machine. Starts from 0 to\n",
    "            num_shards - 1. If single machine is used, then set shard id to 0.\n",
    "        num_shards (int): number of shards using by the job.\n",
    "        init_method (str): initialization method to launch the job with multiple\n",
    "            devices. Options includes TCP or shared file-system for\n",
    "            initialization. details can be find in\n",
    "            https://pytorch.org/docs/stable/distributed.html#tcp-initialization\n",
    "        cfg (str): path to the config file.\n",
    "        opts (argument): provide addtional options from the command line, it\n",
    "            overwrites the config loaded from file.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Provide training and testing pipeline.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shard_id\",\n",
    "        help=\"The shard id of current node, Starts from 0 to num_shards - 1\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_shards\",\n",
    "        help=\"Number of shards using by the job\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--init_method\",\n",
    "        help=\"Initialization method, includes TCP or shared file-system\",\n",
    "        default=\"tcp://localhost:9999\",\n",
    "        type=str,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cfg\",\n",
    "        dest=\"cfg_file\",\n",
    "        help=\"Path to the config file\",\n",
    "        default=\"configs/MVIT_B.yaml\",\n",
    "        type=str,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"opts\",\n",
    "        help=\"See mvit/config/defaults.py for all options\",\n",
    "        default=None,\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    if len(sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_config(args):\n",
    "    \"\"\"\n",
    "    Given the arguemnts, load and initialize the configs.\n",
    "    Args:\n",
    "        args (argument): arguments includes `shard_id`, `num_shards`,\n",
    "            `init_method`, `cfg_file`, and `opts`.\n",
    "    \"\"\"\n",
    "    # Setup cfg.\n",
    "    cfg = get_cfg()\n",
    "    # Load config from cfg.\n",
    "    if args.cfg_file is not None:\n",
    "        cfg.merge_from_file(args.cfg_file)\n",
    "    # Load config from command line, overwrite config from opts.\n",
    "    if args.opts is not None:\n",
    "        cfg.merge_from_list(args.opts)\n",
    "\n",
    "    # Inherit parameters from args.\n",
    "    if hasattr(args, \"num_shards\") and hasattr(args, \"shard_id\"):\n",
    "        cfg.NUM_SHARDS = args.num_shards\n",
    "        cfg.SHARD_ID = args.shard_id\n",
    "    if hasattr(args, \"rng_seed\"):\n",
    "        cfg.RNG_SEED = args.rng_seed\n",
    "    if hasattr(args, \"output_dir\"):\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "\n",
    "    # Create the checkpoint dir.\n",
    "    cu.make_checkpoint_dir(cfg.OUTPUT_DIR)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7758e4ba-6635-4bf1-a6c9-258f24e6e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc823f55-6501-4ca9-9f27-d4c379bdd654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'imagenet', 'BATCH_SIZE': 256, 'EVAL_PERIOD': 10, 'CHECKPOINT_PERIOD': 10, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_EPOCH_RESET': False, 'MIXED_PRECISION': False}), 'AUG': CfgNode({'NUM_SAMPLE': 1, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m9-n6-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': False, 'DATASET': 'imagenet', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_SQUEEZE_TEMPORAL': True}), 'MODEL': CfgNode({'MODEL_NAME': 'MViT', 'NUM_CLASSES': 1000, 'LOSS_FUNC': 'soft_cross_entropy', 'DROPOUT_RATE': 0.0, 'HEAD_ACT': 'softmax', 'ACT_CHECKPOINT': False}), 'MVIT': CfgNode({'MODE': 'conv', 'POOL_FIRST': False, 'CLS_EMBED_ON': False, 'PATCH_KERNEL': [7, 7], 'PATCH_STRIDE': [4, 4], 'PATCH_PADDING': [3, 3], 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': None, 'POOL_KV_STRIDE_ADAPTIVE': None, 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': (3, 3), 'ZERO_DECAY_POS_CLS': False, 'USE_ABS_POS': False, 'REL_POS_SPATIAL': True, 'REL_POS_ZERO_INIT': False, 'RESIDUAL_POOLING': True, 'DIM_MUL_IN_ATT': True}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': '', 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'VAL_CROP_RATIO': 0.875, 'IN22K_TRAINVAL': False, 'IN22k_VAL_IN1K': ''}), 'SOLVER': CfgNode({'BASE_LR': 0.00025, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 300, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 70.0, 'WARMUP_START_LR': 1e-08, 'OPTIMIZING_METHOD': 'sgd', 'BASE_LR_SCALE_NUM_SHARDS': False, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRAD_VAL': None, 'CLIP_GRAD_L2NORM': None, 'LAYER_DECAY': 1.0}), 'NUM_GPUS': 8, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './tmp', 'RNG_SEED': 0, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True})})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulled in config file manually - this is what's in the defaults \n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2371f000-8b23-4648-92c1-10477f58e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change gpus to zero first \n",
    "cfg['NUM_GPUS'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f75c79-74ad-44ff-875e-97e26b10302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch num_classes to whatever's relevant for us - let's say 5 for now \n",
    "#cfg.MODEL.NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971e5cc7-697f-4767-b3b0-23584c38ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code from mvit_model.py (within the mvit/models folder. Run this whole thing \n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mvit.models.attention import MultiScaleBlock\n",
    "from mvit.models.common import round_width\n",
    "from mvit.utils.misc import validate_checkpoint_wrapper_import\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "\n",
    "try:\n",
    "    from fairscale.nn.checkpoint import checkpoint_wrapper\n",
    "except ImportError:\n",
    "    checkpoint_wrapper = None\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchEmbed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in=3,\n",
    "        dim_out=768,\n",
    "        kernel=(7, 7),\n",
    "        stride=(4, 4),\n",
    "        padding=(3, 3),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            dim_in,\n",
    "            dim_out,\n",
    "            kernel_size=kernel,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        # B C H W -> B HW C\n",
    "        return x.flatten(2).transpose(1, 2), x.shape\n",
    "\n",
    "\n",
    "class TransformerBasicHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Transformer Head. No pool.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        num_classes,\n",
    "        dropout_rate=0.0,\n",
    "        act_func=\"softmax\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform linear projection and activation as head for tranformers.\n",
    "        Args:\n",
    "            dim_in (int): the channel dimension of the input to the head.\n",
    "            num_classes (int): the channel dimensions of the output to the head.\n",
    "            dropout_rate (float): dropout rate. If equal to 0.0, perform no\n",
    "                dropout.\n",
    "            act_func (string): activation function to use. 'softmax': applies\n",
    "                softmax on the output. 'sigmoid': applies sigmoid on the output.\n",
    "        \"\"\"\n",
    "        super(TransformerBasicHead, self).__init__()\n",
    "        if dropout_rate > 0.0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.projection = nn.Linear(dim_in, num_classes, bias=True)\n",
    "\n",
    "        # Softmax for evaluation and testing.\n",
    "        if act_func == \"softmax\":\n",
    "            self.act = nn.Softmax(dim=1)\n",
    "        elif act_func == \"sigmoid\":\n",
    "            self.act = nn.Sigmoid()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"{} is not supported as an activation\" \"function.\".format(act_func)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            x = self.dropout(x)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        if not self.training:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@MODEL_REGISTRY.register()\n",
    "class MViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved Multiscale Vision Transformers for Classification and Detection\n",
    "    Yanghao Li*, Chao-Yuan Wu*, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik,\n",
    "        Christoph Feichtenhofer*\n",
    "    https://arxiv.org/abs/2112.01526\n",
    "    Multiscale Vision Transformers\n",
    "    Haoqi Fan*, Bo Xiong*, Karttikeya Mangalam*, Yanghao Li*, Zhicheng Yan, Jitendra Malik,\n",
    "        Christoph Feichtenhofer*\n",
    "    https://arxiv.org/abs/2104.11227\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Get parameters.\n",
    "        assert cfg.DATA.TRAIN_CROP_SIZE == cfg.DATA.TEST_CROP_SIZE\n",
    "        # Prepare input.\n",
    "        in_chans = 3\n",
    "        spatial_size = cfg.DATA.TRAIN_CROP_SIZE\n",
    "        # Prepare output.\n",
    "        num_classes = cfg.MODEL.NUM_CLASSES\n",
    "        embed_dim = cfg.MVIT.EMBED_DIM\n",
    "        # MViT params.\n",
    "        num_heads = cfg.MVIT.NUM_HEADS\n",
    "        depth = cfg.MVIT.DEPTH\n",
    "        self.cls_embed_on = cfg.MVIT.CLS_EMBED_ON\n",
    "        self.use_abs_pos = cfg.MVIT.USE_ABS_POS\n",
    "        self.zero_decay_pos_cls = cfg.MVIT.ZERO_DECAY_POS_CLS\n",
    "\n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        if cfg.MODEL.ACT_CHECKPOINT:\n",
    "            validate_checkpoint_wrapper_import(checkpoint_wrapper)\n",
    "\n",
    "        patch_embed = PatchEmbed(\n",
    "            dim_in=in_chans,\n",
    "            dim_out=embed_dim,\n",
    "            kernel=cfg.MVIT.PATCH_KERNEL,\n",
    "            stride=cfg.MVIT.PATCH_STRIDE,\n",
    "            padding=cfg.MVIT.PATCH_PADDING,\n",
    "        )\n",
    "        if cfg.MODEL.ACT_CHECKPOINT:\n",
    "            patch_embed = checkpoint_wrapper(patch_embed)\n",
    "        self.patch_embed = patch_embed\n",
    "\n",
    "        patch_dims = [\n",
    "            spatial_size // cfg.MVIT.PATCH_STRIDE[0],\n",
    "            spatial_size // cfg.MVIT.PATCH_STRIDE[1],\n",
    "        ]\n",
    "        num_patches = math.prod(patch_dims)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, cfg.MVIT.DROPPATH_RATE, depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "\n",
    "        if self.cls_embed_on:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "            pos_embed_dim = num_patches + 1\n",
    "        else:\n",
    "            pos_embed_dim = num_patches\n",
    "\n",
    "        if self.use_abs_pos:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, pos_embed_dim, embed_dim))\n",
    "\n",
    "        # MViT backbone configs\n",
    "        dim_mul, head_mul, pool_q, pool_kv, stride_q, stride_kv = _prepare_mvit_configs(\n",
    "            cfg\n",
    "        )\n",
    "\n",
    "        input_size = patch_dims\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            num_heads = round_width(num_heads, head_mul[i])\n",
    "            if cfg.MVIT.DIM_MUL_IN_ATT:\n",
    "                dim_out = round_width(\n",
    "                    embed_dim,\n",
    "                    dim_mul[i],\n",
    "                    divisor=round_width(num_heads, head_mul[i]),\n",
    "                )\n",
    "            else:\n",
    "                dim_out = round_width(\n",
    "                    embed_dim,\n",
    "                    dim_mul[i + 1],\n",
    "                    divisor=round_width(num_heads, head_mul[i + 1]),\n",
    "                )\n",
    "            attention_block = MultiScaleBlock(\n",
    "                dim=embed_dim,\n",
    "                dim_out=dim_out,\n",
    "                num_heads=num_heads,\n",
    "                input_size=input_size,\n",
    "                mlp_ratio=cfg.MVIT.MLP_RATIO,\n",
    "                qkv_bias=cfg.MVIT.QKV_BIAS,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                kernel_q=pool_q[i] if len(pool_q) > i else [],\n",
    "                kernel_kv=pool_kv[i] if len(pool_kv) > i else [],\n",
    "                stride_q=stride_q[i] if len(stride_q) > i else [],\n",
    "                stride_kv=stride_kv[i] if len(stride_kv) > i else [],\n",
    "                mode=cfg.MVIT.MODE,\n",
    "                has_cls_embed=self.cls_embed_on,\n",
    "                pool_first=cfg.MVIT.POOL_FIRST,\n",
    "                rel_pos_spatial=cfg.MVIT.REL_POS_SPATIAL,\n",
    "                rel_pos_zero_init=cfg.MVIT.REL_POS_ZERO_INIT,\n",
    "                residual_pooling=cfg.MVIT.RESIDUAL_POOLING,\n",
    "                dim_mul_in_att=cfg.MVIT.DIM_MUL_IN_ATT,\n",
    "            )\n",
    "\n",
    "            if cfg.MODEL.ACT_CHECKPOINT:\n",
    "                attention_block = checkpoint_wrapper(attention_block)\n",
    "            self.blocks.append(attention_block)\n",
    "\n",
    "            if len(stride_q[i]) > 0:\n",
    "                input_size = [\n",
    "                    size // stride for size, stride in zip(input_size, stride_q[i])\n",
    "                ]\n",
    "            embed_dim = dim_out\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.head = TransformerBasicHead(\n",
    "            embed_dim,\n",
    "            num_classes,\n",
    "            dropout_rate=cfg.MODEL.DROPOUT_RATE,\n",
    "            act_func=cfg.MODEL.HEAD_ACT,\n",
    "        )\n",
    "        if self.use_abs_pos:\n",
    "            trunc_normal_(self.pos_embed, std=0.02)\n",
    "        if self.cls_embed_on:\n",
    "            trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        names = []\n",
    "        if self.zero_decay_pos_cls:\n",
    "            # add all potential params\n",
    "            names = [\"pos_embed\", \"rel_pos_h\", \"rel_pos_w\", \"cls_token\"]\n",
    "\n",
    "        return names\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, bchw = self.patch_embed(x)\n",
    "\n",
    "        H, W = bchw[-2], bchw[-1]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        if self.cls_embed_on:\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        if self.use_abs_pos:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "        thw = [H, W]\n",
    "        for blk in self.blocks:\n",
    "            x, thw = blk(x, thw)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        #if self.cls_embed_on:\n",
    "        #    x = x[:, 0]\n",
    "        #else:\n",
    "        #    x = x.mean(1)\n",
    "#\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _prepare_mvit_configs(cfg):\n",
    "    \"\"\"\n",
    "    Prepare mvit configs for dim_mul and head_mul facotrs, and q and kv pooling\n",
    "    kernels and strides.\n",
    "    \"\"\"\n",
    "    depth = cfg.MVIT.DEPTH\n",
    "    dim_mul, head_mul = torch.ones(depth + 1), torch.ones(depth + 1)\n",
    "    for i in range(len(cfg.MVIT.DIM_MUL)):\n",
    "        dim_mul[cfg.MVIT.DIM_MUL[i][0]] = cfg.MVIT.DIM_MUL[i][1]\n",
    "    for i in range(len(cfg.MVIT.HEAD_MUL)):\n",
    "        head_mul[cfg.MVIT.HEAD_MUL[i][0]] = cfg.MVIT.HEAD_MUL[i][1]\n",
    "\n",
    "    pool_q = [[] for i in range(depth)]\n",
    "    pool_kv = [[] for i in range(depth)]\n",
    "    stride_q = [[] for i in range(depth)]\n",
    "    stride_kv = [[] for i in range(depth)]\n",
    "\n",
    "    for i in range(len(cfg.MVIT.POOL_Q_STRIDE)):\n",
    "        stride_q[cfg.MVIT.POOL_Q_STRIDE[i][0]] = cfg.MVIT.POOL_Q_STRIDE[i][1:]\n",
    "        pool_q[cfg.MVIT.POOL_Q_STRIDE[i][0]] = cfg.MVIT.POOL_KVQ_KERNEL\n",
    "\n",
    "    # If POOL_KV_STRIDE_ADAPTIVE is not None, initialize POOL_KV_STRIDE.\n",
    "    if cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE is not None:\n",
    "        _stride_kv = cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE\n",
    "        cfg.MVIT.POOL_KV_STRIDE = []\n",
    "        for i in range(cfg.MVIT.DEPTH):\n",
    "            if len(stride_q[i]) > 0:\n",
    "                _stride_kv = [\n",
    "                    max(_stride_kv[d] // stride_q[i][d], 1)\n",
    "                    for d in range(len(_stride_kv))\n",
    "                ]\n",
    "            cfg.MVIT.POOL_KV_STRIDE.append([i] + _stride_kv)\n",
    "\n",
    "    for i in range(len(cfg.MVIT.POOL_KV_STRIDE)):\n",
    "        stride_kv[cfg.MVIT.POOL_KV_STRIDE[i][0]] = cfg.MVIT.POOL_KV_STRIDE[i][1:]\n",
    "        pool_kv[cfg.MVIT.POOL_KV_STRIDE[i][0]] = cfg.MVIT.POOL_KVQ_KERNEL\n",
    "\n",
    "    return dim_mul, head_mul, pool_q, pool_kv, stride_q,  stride_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9814ef04-a647-4679-b511-40b48a0b424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason you have to change this from none to empty list to work\n",
    "cfg.MVIT.POOL_KV_STRIDE =[]\n",
    "# take the manual config files from the MVITv2_T.yaml file and set them here for testing\n",
    "cfg.MVIT.DROPPATH_RATE= 0.1\n",
    "cfg.MVIT.DEPTH= 10\n",
    "cfg.MVIT.DIM_MUL= [[1, 2.0], [3, 2.0], [8, 2.0]]\n",
    "cfg.MVIT.HEAD_MUL= [[1, 2.0], [3, 2.0], [8, 2.0]]\n",
    "cfg.MVIT.POOL_KVQ_KERNEL= [3, 3]\n",
    "cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE= [4, 4]\n",
    "cfg.MVIT.POOL_Q_STRIDE= [[0, 1, 1], [1, 2, 2], [2, 1, 1], [3, 2, 2], [4, 1, 1], [5, 1, 1], [6, 1, 1], [7, 1, 1], [8, 2, 2], [9, 1, 1]]\n",
    "cfg.SOLVER.BASE_LR_SCALE_NUM_SHARDS= True\n",
    "cfg.SOLVER.BASE_LR= 0.00025\n",
    "cfg.SOLVER.LR_POLICY= 'cosine'\n",
    "cfg.SOLVER.MAX_EPOCH= 300\n",
    "cfg.SOLVER.WEIGHT_DECAY= 0.05\n",
    "cfg.SOLVER.OPTIMIZING_METHOD= 'adamw'\n",
    "cfg.SOLVER.CLIP_GRAD_L2NORM= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b3e611-9f53-48f4-8a4b-d480706b28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we instantiate the MViT class, and in doing so register the model \n",
    "model_curr = MViT(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7737e31b-0704-4345-8ead-dc709a64fabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Programs\\anaconda\\envs\\hyka_ml\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Cuda is not available. Please set `NUM_GPUS: 0 for running on CPUs.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [7], line 15\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(cfg, gpu_id)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m         cfg\u001b[38;5;241m.\u001b[39mNUM_GPUS \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m     13\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use more GPU devices than available\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         cfg\u001b[38;5;241m.\u001b[39mNUM_GPUS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCuda is not available. Please set `NUM_GPUS: 0 for running on CPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Construct the model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m name \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mMODEL_NAME\n",
      "\u001b[1;31mAssertionError\u001b[0m: Cuda is not available. Please set `NUM_GPUS: 0 for running on CPUs."
     ]
    }
   ],
   "source": [
    "test = build_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a622f-5e8e-480a-97ca-00bcf56ff4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Registry of MODEL:\n",
       "╒═════════╤═════════════════════════╕\n",
       "│ Names   │ Objects                 │\n",
       "╞═════════╪═════════════════════════╡\n",
       "│ MViT    │ <class '__main__.MViT'> │\n",
       "╘═════════╧═════════════════════════╛"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we can see, model is now registered - this means that training should technically work because the model exists now \n",
    "MODEL_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a217f-9b99-4180-9326-dec56a5524f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just need to figure out how to load the pretrained weights into this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434fcf4-fb24-4549-a644-02946037f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:\\OneDrive - The University of Sydney (Students)\\DATA5703 Capstone Project (Group)\\Scripts\\usyd_cs10_2\\MVIT\n",
      " Volume in drive H is Internal Storage\n",
      " Volume Serial Number is 288B-93B9\n",
      "\n",
      " Directory of h:\\OneDrive - The University of Sydney (Students)\\DATA5703 Capstone Project (Group)\\Scripts\\usyd_cs10_2\\MVIT\n",
      "\n",
      "18/09/2022  05:46 PM    <DIR>          .\n",
      "18/09/2022  05:46 PM    <DIR>          ..\n",
      "17/09/2022  11:02 PM               480 .flake8\n",
      "21/09/2022  01:32 AM    <DIR>          archive\n",
      "18/09/2022  05:37 PM        20,540,149 archive.zip\n",
      "17/09/2022  11:02 PM             3,615 CODE_OF_CONDUCT.md\n",
      "18/09/2022  04:28 PM    <DIR>          configs\n",
      "17/09/2022  11:02 PM             1,472 CONTRIBUTING.md\n",
      "17/09/2022  11:02 PM             1,116 INSTALL.md\n",
      "17/09/2022  11:02 PM            10,443 LICENSE\n",
      "17/09/2022  11:02 PM               680 linter.sh\n",
      "18/09/2022  04:28 PM    <DIR>          mvit\n",
      "17/09/2022  11:18 PM       290,305,429 MViTv2_T_in1k.pyth\n",
      "17/09/2022  11:02 PM             3,542 README.md\n",
      "17/09/2022  11:02 PM               753 setup.cfg\n",
      "17/09/2022  11:02 PM               573 setup.py\n",
      "18/09/2022  04:28 PM    <DIR>          tools\n",
      "              11 File(s)    310,868,252 bytes\n",
      "               6 Dir(s)  344,236,380,160 bytes free\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b9ba1-67a7-41cb-bc3e-d3aacfc5d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mvit.utils.distributed as du\n",
    "import mvit.utils.logging as logging\n",
    "import torch\n",
    "from mvit.utils.env import checkpoint_pathmgr as pathmgr\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122240ac-b1ec-4af0-9158-ceec60bec1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    path_to_checkpoint,\n",
    "    model,\n",
    "    data_parallel=False,\n",
    "    optimizer=None,\n",
    "    scaler=None,\n",
    "    epoch_reset=False,\n",
    "    squeeze_temporal=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the checkpoint from the given file.\n",
    "    Args:\n",
    "        path_to_checkpoint (string): path to the checkpoint to load.\n",
    "        model (model): model to load the weights from the checkpoint.\n",
    "        data_parallel (bool): if true, model is wrapped by\n",
    "        torch.nn.parallel.DistributedDataParallel.\n",
    "        optimizer (optim): optimizer to load the historical state.\n",
    "        scaler (GradScaler): GradScaler to load the mixed precision scale.\n",
    "        epoch_reset (bool): if True, reset #train iterations from the checkpoint.\n",
    "        squeeze_temporal (bool): if True, squeeze temporal dimension for 3D conv to\n",
    "            2D conv.\n",
    "    Returns:\n",
    "        (int): the number of training epoch of the checkpoint.\n",
    "    \"\"\"\n",
    "    assert pathmgr.exists(path_to_checkpoint), \"Checkpoint '{}' not found\".format(\n",
    "        path_to_checkpoint\n",
    "    )\n",
    "    logger.info(\"Loading network weights from {}.\".format(path_to_checkpoint))\n",
    "\n",
    "    # Account for the DDP wrapper in the multi-gpu setting.\n",
    "    ms = model.module if data_parallel else model\n",
    "\n",
    "    # Load the checkpoint on CPU to avoid GPU mem spike.\n",
    "    with pathmgr.open(path_to_checkpoint, \"rb\") as f:\n",
    "        checkpoint = torch.load(f, map_location=\"cpu\")\n",
    "\n",
    "    pre_train_dict = checkpoint[\"model_state\"]\n",
    "    model_dict = ms.state_dict()\n",
    "\n",
    "    if squeeze_temporal:\n",
    "        for k, v in pre_train_dict.items():\n",
    "            # convert 3D conv to 2D\n",
    "            if (\n",
    "                k in model_dict\n",
    "                and len(v.size()) == 5\n",
    "                and len(model_dict[k].size()) == 4\n",
    "                and v.size()[2] == 1\n",
    "            ):\n",
    "                pre_train_dict[k] = v.squeeze(2)\n",
    "\n",
    "    # Match pre-trained weights that have same shape as current model.\n",
    "    pre_train_dict_match = {\n",
    "        k: v\n",
    "        for k, v in pre_train_dict.items()\n",
    "        if k in model_dict and v.size() == model_dict[k].size()\n",
    "    }\n",
    "    # Weights that do not have match from the pre-trained model.\n",
    "    not_load_layers = [\n",
    "        k for k in model_dict.keys() if k not in pre_train_dict_match.keys()\n",
    "    ]\n",
    "    # Log weights that are not loaded with the pre-trained weights.\n",
    "    if not_load_layers:\n",
    "        for k in not_load_layers:\n",
    "            logger.info(\"Network weights {} not loaded.\".format(k))\n",
    "    # Weights that do not have match from the pre-trained model.\n",
    "    not_use_layers = [\n",
    "        k for k in pre_train_dict.keys() if k not in pre_train_dict_match.keys()\n",
    "    ]\n",
    "    # Log weights that are not loaded with the pre-trained weights.\n",
    "    if not_use_layers:\n",
    "        for k in not_use_layers:\n",
    "            logger.info(\"Network weights {} not used.\".format(k))\n",
    "    # Load pre-trained weights.\n",
    "    ms.load_state_dict(pre_train_dict_match, strict=False)\n",
    "    epoch = -1\n",
    "\n",
    "    # Load the optimizer state (commonly not done when fine-tuning)\n",
    "    if \"epoch\" in checkpoint.keys() and not epoch_reset:\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        if scaler:\n",
    "            scaler.load_state_dict(checkpoint[\"scaler_state\"])\n",
    "    else:\n",
    "        epoch = -1\n",
    "    return epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae1ea3-a3e6-4bbb-bee2-525c07290054",
   "metadata": {},
   "source": [
    "#### 3. Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221103b-abb2-422c-9ab9-78646be531bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model object - need to download model object first from github \n",
    "# note that this pretrained object has mutliple things in it - need to figure out how to get the weights into the model from above \n",
    "curr_model = MViT(cfg)\n",
    "pretrained = torch.load('MViTv2_T_in1k.pyth')\n",
    "path_to_checkpoint = \"MViTv2_T_in1k.pyth\"\n",
    "# running this wil load the pre-trained model and will also return the starting epoch whatever that means \n",
    "load_checkpoint(path_to_checkpoint=path_to_checkpoint, model=curr_model)\n",
    "cfg.TRAIN.CHECKPOINT_FILE_PATH = \"MViTv2_T_in1k.pyth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff6491-8964-41b4-8197-ada369e00b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from fvcore.common.registry import Registry\n",
    "\n",
    "DATASET_REGISTRY = Registry(\"DATASET\")\n",
    "DATASET_REGISTRY.__doc__ = \"\"\"\n",
    "Registry for dataset.\n",
    "The registered object will be called with `obj(cfg, split)`.\n",
    "The call should return a `torch.utils.data.Dataset` object.\n",
    "\"\"\"\n",
    "\n",
    "def build_dataset(dataset_name, cfg, split):\n",
    "    \"\"\"\n",
    "    Build a dataset, defined by `dataset_name`.\n",
    "    Args:\n",
    "        dataset_name (str): the name of the dataset to be constructed.\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "        split (str): the split of the data loader. Options include `train`,\n",
    "            `val`, and `test`.\n",
    "    Returns:\n",
    "        Dataset: a constructed dataset specified by dataset_name.\n",
    "    \"\"\"\n",
    "    # Capitalize the the first letter of the dataset_name since the dataset_name\n",
    "    # in configs may be in lowercase but the name of dataset class should always\n",
    "    # start with an uppercase letter.\n",
    "    name = dataset_name.capitalize()\n",
    "    return DATASET_REGISTRY.get(name)(cfg, split)\n",
    "\n",
    "def multiple_samples_collate(batch):\n",
    "    \"\"\"\n",
    "    Collate function for repeated augmentation. Each instance in the batch has\n",
    "    more than one sample.\n",
    "    Args:\n",
    "        batch (tuple or list): data batch to collate.\n",
    "    Returns:\n",
    "        (tuple): collated data batch.\n",
    "    \"\"\"\n",
    "    inputs, labels = zip(*batch)\n",
    "    inputs = [item for sublist in inputs for item in sublist]\n",
    "    labels = [item for sublist in labels for item in sublist]\n",
    "\n",
    "    inputs, labels = default_collate(inputs), default_collate(labels)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def construct_loader(cfg, split):\n",
    "    \"\"\"\n",
    "    Constructs the data loader for the given dataset.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "        split (str): the split of the data loader. Options include `train`,\n",
    "            `val`, and `test`.\n",
    "    \"\"\"\n",
    "    assert split in [\"train\", \"val\", \"test\"]\n",
    "    if split in [\"train\"]:\n",
    "        dataset_name = cfg.TRAIN.DATASET\n",
    "        batch_size = int(cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS))\n",
    "        shuffle = True\n",
    "        drop_last = True\n",
    "    elif split in [\"val\"]:\n",
    "        dataset_name = cfg.TRAIN.DATASET\n",
    "        batch_size = int(cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS))\n",
    "        shuffle = False\n",
    "        drop_last = False\n",
    "    elif split in [\"test\"]:\n",
    "        dataset_name = cfg.TEST.DATASET\n",
    "        batch_size = int(cfg.TEST.BATCH_SIZE / max(1, cfg.NUM_GPUS))\n",
    "        shuffle = False\n",
    "        drop_last = False\n",
    "\n",
    "    # Construct the dataset\n",
    "    dataset = build_dataset(dataset_name, cfg, split)\n",
    "\n",
    "    # Create a sampler for multi-process training\n",
    "    sampler = DistributedSampler(dataset) if cfg.NUM_GPUS > 1 else None\n",
    "\n",
    "    if cfg.AUG.NUM_SAMPLE > 1 and split in [\"train\"]:\n",
    "        collate_func = multiple_samples_collate\n",
    "    else:\n",
    "        collate_func = None\n",
    "\n",
    "    # Create a loader\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(False if sampler else shuffle),\n",
    "        sampler=sampler,\n",
    "        num_workers=cfg.DATA_LOADER.NUM_WORKERS,\n",
    "        pin_memory=cfg.DATA_LOADER.PIN_MEMORY,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=collate_func,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "\n",
    "def shuffle_dataset(loader, cur_epoch):\n",
    "    \"\"\" \"\n",
    "    Shuffles the dataset.\n",
    "    Args:\n",
    "        loader (loader): data loader to perform shuffle.\n",
    "        cur_epoch (int): number of the current epoch.\n",
    "    \"\"\"\n",
    "    sampler = loader.sampler\n",
    "    assert isinstance(\n",
    "        sampler, (RandomSampler, DistributedSampler)\n",
    "    ), \"Sampler type '{}' not supported\".format(type(sampler))\n",
    "    # RandomSampler handles shuffling automatically\n",
    "    if isinstance(sampler, DistributedSampler):\n",
    "        # DistributedSampler shuffles data based on epoch\n",
    "        sampler.set_epoch(cur_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d97082-7d32-4fef-82da-7521b0e7129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:14: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  Image.NEAREST: \"PIL.Image.NEAREST\",\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:15: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  Image.BILINEAR: \"PIL.Image.BILINEAR\",\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:16: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  Image.BICUBIC: \"PIL.Image.BICUBIC\",\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:17: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  Image.LANCZOS: \"PIL.Image.LANCZOS\",\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:18: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  Image.HAMMING: \"PIL.Image.HAMMING\",\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:19: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  Image.BOX: \"PIL.Image.BOX\",\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:22: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11696\\2421098515.py:22: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n"
     ]
    }
   ],
   "source": [
    "#transform.py\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from mvit.datasets.rand_augment import rand_augment_transform # edited location\n",
    "from mvit.datasets.random_erasing import RandomErasing # edited location\n",
    "\n",
    "_pil_interpolation_to_str = {\n",
    "    Image.NEAREST: \"PIL.Image.NEAREST\",\n",
    "    Image.BILINEAR: \"PIL.Image.BILINEAR\",\n",
    "    Image.BICUBIC: \"PIL.Image.BICUBIC\",\n",
    "    Image.LANCZOS: \"PIL.Image.LANCZOS\",\n",
    "    Image.HAMMING: \"PIL.Image.HAMMING\",\n",
    "    Image.BOX: \"PIL.Image.BOX\",\n",
    "}\n",
    "\n",
    "_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n",
    "\n",
    "\n",
    "def _pil_interp(method):\n",
    "    if method == \"bicubic\":\n",
    "        return Image.BICUBIC\n",
    "    elif method == \"lanczos\":\n",
    "        return Image.LANCZOS\n",
    "    elif method == \"hamming\":\n",
    "        return Image.HAMMING\n",
    "    else:\n",
    "        return Image.BILINEAR\n",
    "\n",
    "\n",
    "# The following code are modified based on timm lib, we will replace the following\n",
    "# contents with dependency from PyTorchVideo.\n",
    "# https://github.com/facebookresearch/pytorchvideo\n",
    "class RandomResizedCropAndInterpolation:\n",
    "    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n",
    "    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n",
    "    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n",
    "    is finally resized to given size.\n",
    "    This is popularly used to train the Inception networks.\n",
    "    Args:\n",
    "        size: expected output size of each edge\n",
    "        scale: range of size of the origin size cropped\n",
    "        ratio: range of aspect ratio of the origin aspect ratio cropped\n",
    "        interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size,\n",
    "        scale=(0.08, 1.0),\n",
    "        ratio=(3.0 / 4.0, 4.0 / 3.0),\n",
    "        interpolation=\"bilinear\",\n",
    "    ):\n",
    "        if isinstance(size, tuple):\n",
    "            self.size = size\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n",
    "            print(\"range should be of kind (min, max)\")\n",
    "\n",
    "        if interpolation == \"random\":\n",
    "            self.interpolation = _RANDOM_INTERPOLATION\n",
    "        else:\n",
    "            self.interpolation = _pil_interp(interpolation)\n",
    "        self.scale = scale\n",
    "        self.ratio = ratio\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img, scale, ratio):\n",
    "        \"\"\"Get parameters for ``crop`` for a random sized crop.\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "            scale (tuple): range of size of the origin size cropped\n",
    "            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n",
    "        Returns:\n",
    "            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n",
    "                sized crop.\n",
    "        \"\"\"\n",
    "        area = img.size[0] * img.size[1]\n",
    "\n",
    "        for _ in range(10):\n",
    "            target_area = random.uniform(*scale) * area\n",
    "            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n",
    "            aspect_ratio = math.exp(random.uniform(*log_ratio))\n",
    "\n",
    "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if w <= img.size[0] and h <= img.size[1]:\n",
    "                i = random.randint(0, img.size[1] - h)\n",
    "                j = random.randint(0, img.size[0] - w)\n",
    "                return i, j, h, w\n",
    "\n",
    "        # Fallback to central crop\n",
    "        in_ratio = img.size[0] / img.size[1]\n",
    "        if in_ratio < min(ratio):\n",
    "            w = img.size[0]\n",
    "            h = int(round(w / min(ratio)))\n",
    "        elif in_ratio > max(ratio):\n",
    "            h = img.size[1]\n",
    "            w = int(round(h * max(ratio)))\n",
    "        else:  # whole image\n",
    "            w = img.size[0]\n",
    "            h = img.size[1]\n",
    "        i = (img.size[1] - h) // 2\n",
    "        j = (img.size[0] - w) // 2\n",
    "        return i, j, h, w\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped and resized.\n",
    "        Returns:\n",
    "            PIL Image: Randomly cropped and resized image.\n",
    "        \"\"\"\n",
    "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
    "        if isinstance(self.interpolation, (tuple, list)):\n",
    "            interpolation = random.choice(self.interpolation)\n",
    "        else:\n",
    "            interpolation = self.interpolation\n",
    "        return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if isinstance(self.interpolation, (tuple, list)):\n",
    "            interpolate_str = \" \".join(\n",
    "                [_pil_interpolation_to_str[x] for x in self.interpolation]\n",
    "            )\n",
    "        else:\n",
    "            interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        format_string = self.__class__.__name__ + \"(size={0}\".format(self.size)\n",
    "        format_string += \", scale={0}\".format(tuple(round(s, 4) for s in self.scale))\n",
    "        format_string += \", ratio={0}\".format(tuple(round(r, 4) for r in self.ratio))\n",
    "        format_string += \", interpolation={0})\".format(interpolate_str)\n",
    "        return format_string\n",
    "\n",
    "\n",
    "def transforms_imagenet_train(\n",
    "    img_size=224,\n",
    "    scale=None,\n",
    "    ratio=None,\n",
    "    hflip=0.5,\n",
    "    vflip=0.0,\n",
    "    color_jitter=0.4,\n",
    "    auto_augment=None,\n",
    "    interpolation=\"random\",\n",
    "    use_prefetcher=False,\n",
    "    mean=(0.485, 0.456, 0.406),\n",
    "    std=(0.229, 0.224, 0.225),\n",
    "    re_prob=0.0,\n",
    "    re_mode=\"const\",\n",
    "    re_count=1,\n",
    "    re_num_splits=0,\n",
    "    separate=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n",
    "    for use in a mixing dataset that passes\n",
    "     * all data through the first (primary) transform, called the 'clean' data\n",
    "     * a portion of the data through the secondary transform\n",
    "     * normalizes and converts the branches above with the third, final transform\n",
    "    \"\"\"\n",
    "    if isinstance(img_size, tuple):\n",
    "        img_size = img_size[-2:]\n",
    "    else:\n",
    "        img_size = img_size\n",
    "\n",
    "    scale = tuple(scale or (0.08, 1.0))  # default imagenet scale range\n",
    "    ratio = tuple(ratio or (3.0 / 4.0, 4.0 / 3.0))  # default imagenet ratio range\n",
    "    primary_tfl = [\n",
    "        RandomResizedCropAndInterpolation(\n",
    "            img_size, scale=scale, ratio=ratio, interpolation=interpolation\n",
    "        )\n",
    "    ]\n",
    "    if hflip > 0.0:\n",
    "        primary_tfl += [transforms.RandomHorizontalFlip(p=hflip)]\n",
    "    if vflip > 0.0:\n",
    "        primary_tfl += [transforms.RandomVerticalFlip(p=vflip)]\n",
    "\n",
    "    secondary_tfl = []\n",
    "    if auto_augment:\n",
    "        assert isinstance(auto_augment, str)\n",
    "        if isinstance(img_size, tuple):\n",
    "            img_size_min = min(img_size)\n",
    "        else:\n",
    "            img_size_min = img_size\n",
    "        aa_params = dict(\n",
    "            translate_const=int(img_size_min * 0.45),\n",
    "            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n",
    "        )\n",
    "        if interpolation and interpolation != \"random\":\n",
    "            aa_params[\"interpolation\"] = _pil_interp(interpolation)\n",
    "        if auto_augment.startswith(\"rand\"):\n",
    "            secondary_tfl += [rand_augment_transform(auto_augment, aa_params)]\n",
    "        elif auto_augment.startswith(\"augmix\"):\n",
    "            raise NotImplementedError(\"Augmix not implemented\")\n",
    "        else:\n",
    "            raise NotImplementedError(\"Auto aug not implemented\")\n",
    "    elif color_jitter is not None:\n",
    "        # color jitter is enabled when not using AA\n",
    "        if isinstance(color_jitter, (list, tuple)):\n",
    "            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation\n",
    "            # or 4 if also augmenting hue\n",
    "            assert len(color_jitter) in (3, 4)\n",
    "        else:\n",
    "            # if it's a scalar, duplicate for brightness, contrast, and saturation, no hue\n",
    "            color_jitter = (float(color_jitter),) * 3\n",
    "        secondary_tfl += [transforms.ColorJitter(*color_jitter)]\n",
    "\n",
    "    final_tfl = []\n",
    "    final_tfl += [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=torch.tensor(mean), std=torch.tensor(std)),\n",
    "    ]\n",
    "    if re_prob > 0.0:\n",
    "        final_tfl.append(\n",
    "            RandomErasing(\n",
    "                re_prob,\n",
    "                mode=re_mode,\n",
    "                max_count=re_count,\n",
    "                num_splits=re_num_splits,\n",
    "                device=\"cpu\",\n",
    "                cube=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if separate:\n",
    "        return (\n",
    "            transforms.Compose(primary_tfl),\n",
    "            transforms.Compose(secondary_tfl),\n",
    "            transforms.Compose(final_tfl),\n",
    "        )\n",
    "    else:\n",
    "        return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935bc38-0afe-49c6-ab52-5329037f11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data registry code\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import mvit.utils.logging as logging\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from mvit.utils.env import pathmgr\n",
    "from PIL import Image\n",
    "from torchvision import transforms as transforms_tv\n",
    "\n",
    "from mvit.datasets.transform import transforms_imagenet_train\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "@DATASET_REGISTRY.register()\n",
    "class Imagenet(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg, mode, num_retries=10):\n",
    "        self.num_retries = num_retries\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.data_path = cfg.DATA.PATH_TO_DATA_DIR\n",
    "        assert mode in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"test\",\n",
    "        ], \"Split '{}' not supported for ImageNet\".format(mode)\n",
    "        logger.info(\"Constructing ImageNet {}...\".format(mode))\n",
    "        if cfg.DATA.PATH_TO_PRELOAD_IMDB == \"\":\n",
    "            self._construct_imdb()\n",
    "        else:\n",
    "            self._load_imdb()\n",
    "\n",
    "    def _load_imdb(self):\n",
    "        split_path = os.path.join(\n",
    "            self.cfg.DATA.PATH_TO_PRELOAD_IMDB,\n",
    "            f\"{self.mode}.json\" if self.mode != \"test\" else \"val.json\",\n",
    "        )\n",
    "        with pathmgr.open(split_path, \"r\") as f:\n",
    "            data = f.read()\n",
    "        self._imdb = json.loads(data)\n",
    "\n",
    "    def _construct_imdb(self):\n",
    "        \"\"\"Constructs the imdb.\"\"\"\n",
    "        # Compile the split data path\n",
    "        split_path = os.path.join(self.data_path, self.mode)\n",
    "        logger.info(\"{} data path: {}\".format(self.mode, split_path))\n",
    "        # Images are stored per class in subdirs (format: n<number>)\n",
    "        split_files = pathmgr.ls(split_path)\n",
    "        self._class_ids = sorted(f for f in split_files if re.match(r\"^n[0-9]+$\", f))\n",
    "        # Map ImageNet class ids to contiguous ids\n",
    "        self._class_id_cont_id = {v: i for i, v in enumerate(self._class_ids)}\n",
    "        # Construct the image db\n",
    "        self._imdb = []\n",
    "        for class_id in self._class_ids:\n",
    "            cont_id = self._class_id_cont_id[class_id]\n",
    "            im_dir = os.path.join(split_path, class_id)\n",
    "            for im_name in pathmgr.ls(im_dir):\n",
    "                im_path = os.path.join(im_dir, im_name)\n",
    "                self._imdb.append({\"im_path\": im_path, \"class\": cont_id})\n",
    "        logger.info(\"Number of images: {}\".format(len(self._imdb)))\n",
    "        logger.info(\"Number of classes: {}\".format(len(self._class_ids)))\n",
    "\n",
    "    def _prepare_im(self, im_path):\n",
    "        with pathmgr.open(im_path, \"rb\") as f:\n",
    "            with Image.open(f) as im:\n",
    "                im = im.convert(\"RGB\")\n",
    "        # Convert HWC/BGR/int to HWC/RGB/float format for applying transforms\n",
    "        train_size, test_size = (\n",
    "            self.cfg.DATA.TRAIN_CROP_SIZE,\n",
    "            self.cfg.DATA.TEST_CROP_SIZE,\n",
    "        )\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            aug_transform = transforms_imagenet_train(\n",
    "                img_size=(train_size, train_size),\n",
    "                color_jitter=self.cfg.AUG.COLOR_JITTER,\n",
    "                auto_augment=self.cfg.AUG.AA_TYPE,\n",
    "                interpolation=self.cfg.AUG.INTERPOLATION,\n",
    "                re_prob=self.cfg.AUG.RE_PROB,\n",
    "                re_mode=self.cfg.AUG.RE_MODE,\n",
    "                re_count=self.cfg.AUG.RE_COUNT,\n",
    "                mean=self.cfg.DATA.MEAN,\n",
    "                std=self.cfg.DATA.STD,\n",
    "            )\n",
    "        else:\n",
    "            t = []\n",
    "            if self.cfg.DATA.VAL_CROP_RATIO == 0.0:\n",
    "                t.append(\n",
    "                    transforms_tv.Resize((test_size, test_size), interpolation=3),\n",
    "                )\n",
    "            else:\n",
    "                # size = int((256 / 224) * test_size) # = 1/0.875 * test_size\n",
    "                size = int((1.0 / self.cfg.DATA.VAL_CROP_RATIO) * test_size)\n",
    "                t.append(\n",
    "                    transforms_tv.Resize(\n",
    "                        size, interpolation=3\n",
    "                    ),  # to maintain same ratio w.r.t. 224 images\n",
    "                )\n",
    "                t.append(transforms_tv.CenterCrop(test_size))\n",
    "            t.append(transforms_tv.ToTensor())\n",
    "            t.append(transforms_tv.Normalize(self.cfg.DATA.MEAN, self.cfg.DATA.STD))\n",
    "            aug_transform = transforms_tv.Compose(t)\n",
    "        im = aug_transform(im)\n",
    "        return im\n",
    "\n",
    "    def __load__(self, index):\n",
    "        try:\n",
    "            # Load the image\n",
    "            im_path = self._imdb[index][\"im_path\"]\n",
    "            # Prepare the image for training / testing\n",
    "            if self.mode == \"train\" and self.cfg.AUG.NUM_SAMPLE > 1:\n",
    "                im = []\n",
    "                for _ in range(self.cfg.AUG.NUM_SAMPLE):\n",
    "                    crop = self._prepare_im(im_path)\n",
    "                    im.append(crop)\n",
    "                return im\n",
    "            else:\n",
    "                im = self._prepare_im(im_path)\n",
    "                return im\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # if the current image is corrupted, load a different image.\n",
    "        for _ in range(self.num_retries):\n",
    "            im = self.__load__(index)\n",
    "            # Data corrupted, retry with a different image.\n",
    "            if im is None:\n",
    "                index = random.randint(0, len(self._imdb) - 1)\n",
    "            else:\n",
    "                break\n",
    "        # Retrieve the label\n",
    "        label = self._imdb[index][\"class\"]\n",
    "        if isinstance(im, list):\n",
    "            label = [label for _ in range(len(im))]\n",
    "\n",
    "        return im, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814107e-a84d-47d6-8ef4-4096cfd4980d",
   "metadata": {},
   "source": [
    "#### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b0c59-ebc5-4afc-8a82-14a8ee32fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect path to testdata\n",
    "cfg.DATA.PATH_TO_DATA_DIR = \"archive/imagenet-mini\"\n",
    "#cfg.DATA.PATH_TO_DATA_DIR = \"testdata\" # this is a folder structured testdata/train/class/images... where i've just put a small sample of images \n",
    "# set num workers to 2 \n",
    "cfg.DATA_LOADER.NUM_WORKERS = 1\n",
    "\n",
    "# register dataset and load batch sizes\n",
    "cfg.TRAIN.BATCH_SIZE = 4\n",
    "cfg.TEST.BATCH_SIZE = 4\n",
    "cfg.MIXUP.ENABLE = False\n",
    "imagenettrain = Imagenet(cfg,\"train\")\n",
    "train_loader = construct_loader(cfg, \"train\")\n",
    "val_loader = construct_loader(cfg, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95984e-45ff-4764-a132-fa20ee695857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# think you need to use this function to convert labels if you turn off the mixup function\n",
    "def convert_to_one_hot(targets, num_classes, on_value=1.0, off_value=0.0):\n",
    "    \"\"\"\n",
    "    This function converts target class indices to one-hot vectors, given the\n",
    "    number of classes.\n",
    "    Args:\n",
    "        targets (loader): Class labels.\n",
    "        num_classes (int): Total number of classes.\n",
    "        on_value (float): Target Value for ground truth class.\n",
    "        off_value (float): Target Value for other classes.This value is used for\n",
    "            label smoothing.\n",
    "    \"\"\"\n",
    "\n",
    "    targets = targets.long().view(-1, 1)\n",
    "    return torch.full(\n",
    "        (targets.size()[0], num_classes), off_value, device=targets.device\n",
    "    ).scatter_(1, targets, on_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3d6c5-e6eb-4f99-afe5-1e75df4248b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating these lists to debug inputs in train_epoch function\n",
    "holder_list = []\n",
    "holder_list2= []\n",
    "holder_list3= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ee1cc-1491-44ae-b8e5-a79884292c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our own function for errors\n",
    "def topks_correct(preds, labels, ks):\n",
    "    \"\"\"\n",
    "    Given the predictions, labels, and a list of top-k values, compute the\n",
    "    number of correct predictions for each top-k value.\n",
    "    Args:\n",
    "        preds (array): array of predictions. Dimension is batchsize\n",
    "            N x ClassNum.\n",
    "        labels (array): array of labels. Dimension is batchsize N.\n",
    "        ks (list): list of top-k values. For example, ks = [1, 5] correspods\n",
    "            to top-1 and top-5.\n",
    "    Returns:\n",
    "        topks_correct (list): list of numbers, where the `i`-th entry\n",
    "            corresponds to the number of top-`ks[i]` correct predictions.\n",
    "    \"\"\"\n",
    "    assert preds.size(0) == labels.size(\n",
    "        0\n",
    "    ), \"Batch dim of predictions and labels must match\"\n",
    "    # Find the top max_k predictions for each sample\n",
    "    _top_max_k_vals, top_max_k_inds = torch.topk(\n",
    "        preds, max(ks), dim=1, largest=True, sorted=True\n",
    "    )\n",
    "    _toplabelval, labels = torch.topk(\n",
    "        labels, max([1]), dim=1, largest=True, sorted=True\n",
    "    )\n",
    "\n",
    "    # (batch_size, max_k) -> (max_k, batch_size).\n",
    "    top_max_k_inds = top_max_k_inds.t()\n",
    "    # (batch_size, ) -> (max_k, batch_size).\n",
    "    rep_max_k_labels = labels.view(1, -1).expand_as(top_max_k_inds)\n",
    "    # (i, j) = 1 if top i-th prediction for the j-th sample is correct.\n",
    "    top_max_k_correct = top_max_k_inds.eq(rep_max_k_labels)\n",
    "    # Compute the number of topk correct predictions for each k.\n",
    "    topks_correct = [top_max_k_correct[:k, :].float().sum() for k in ks]\n",
    "    return topks_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db47eef-0c24-4094-800c-415d604e3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our own function for errors\n",
    "def topks_correcttest(preds, labels, ks):\n",
    "    \"\"\"\n",
    "    Given the predictions, labels, and a list of top-k values, compute the\n",
    "    number of correct predictions for each top-k value.\n",
    "    Args:\n",
    "        preds (array): array of predictions. Dimension is batchsize\n",
    "            N x ClassNum.\n",
    "        labels (array): array of labels. Dimension is batchsize N.\n",
    "        ks (list): list of top-k values. For example, ks = [1, 5] correspods\n",
    "            to top-1 and top-5.\n",
    "    Returns:\n",
    "        topks_correct (list): list of numbers, where the `i`-th entry\n",
    "            corresponds to the number of top-`ks[i]` correct predictions.\n",
    "    \"\"\"\n",
    "    assert preds.size(0) == labels.size(\n",
    "        0\n",
    "    ), \"Batch dim of predictions and labels must match\"\n",
    "    # Find the top max_k predictions for each sample\n",
    "    _top_max_k_vals, top_max_k_inds = torch.topk(\n",
    "        preds, max(ks), dim=1, largest=True, sorted=True\n",
    "    )\n",
    "    _toplabelval, labels = torch.topk(\n",
    "        labels, max([1]), dim=1, largest=True, sorted=True\n",
    "    )\n",
    "\n",
    "    # (batch_size, max_k) -> (max_k, batch_size).\n",
    "    top_max_k_inds = top_max_k_inds.t()\n",
    "    # (batch_size, ) -> (max_k, batch_size).\n",
    "    rep_max_k_labels = labels.view(1, -1).expand_as(top_max_k_inds)\n",
    "    # (i, j) = 1 if top i-th prediction for the j-th sample is correct.\n",
    "    top_max_k_correct = top_max_k_inds.eq(rep_max_k_labels)\n",
    "    # Compute the number of topk correct predictions for each k.\n",
    "    topks_correct = [top_max_k_correct[:k, :].float().sum() for k in ks]\n",
    "    return topks_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09645bbc-6f0d-4bd9-b1ff-2409249b27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try train epoch\n",
    "import pprint\n",
    "import mvit.models.losses as losses\n",
    "import mvit.models.optimizer as optim\n",
    "import mvit.utils.checkpoint as cu\n",
    "import mvit.utils.distributed as du\n",
    "import mvit.utils.logging as logging\n",
    "import mvit.utils.metrics as metrics\n",
    "import mvit.utils.misc as misc\n",
    "import numpy as np\n",
    "import torch\n",
    "from mvit.datasets import loader\n",
    "from mvit.datasets.mixup import MixUp\n",
    "from mvit.models import build_model\n",
    "from mvit.utils.meters import EpochTimer, TrainMeter, ValMeter\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "def train_epoch(\n",
    "    train_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    train_meter,\n",
    "    cur_epoch,\n",
    "    cfg,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform the training for one epoch.\n",
    "    Args:\n",
    "        train_loader (loader): training loader.\n",
    "        model (model): the model to train.\n",
    "        optimizer (optim): the optimizer to perform optimization on the model's\n",
    "            parameters.\n",
    "        scaler (GradScaler): the `GradScaler` to help perform the steps of gradient scaling.\n",
    "        train_meter (TrainMeter): training meters to log the training performance.\n",
    "        cur_epoch (int): current epoch of training.\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            mvit/config/defaults.py\n",
    "    \"\"\"\n",
    "    # Enable train mode.\n",
    "    model.train()\n",
    "    train_meter.iter_tic()\n",
    "    data_size = len(train_loader)\n",
    "    \n",
    "    if cfg.MIXUP.ENABLE:\n",
    "        mixup_fn = MixUp(\n",
    "            mixup_alpha=cfg.MIXUP.ALPHA,\n",
    "            cutmix_alpha=cfg.MIXUP.CUTMIX_ALPHA,\n",
    "            mix_prob=cfg.MIXUP.PROB,\n",
    "            switch_prob=cfg.MIXUP.SWITCH_PROB,\n",
    "            label_smoothing=cfg.MIXUP.LABEL_SMOOTH_VALUE,\n",
    "            num_classes=cfg.MODEL.NUM_CLASSES,\n",
    "        )\n",
    "        \n",
    "    pred_holder = torch.empty(size=[0,1000])    \n",
    "    label_holder = torch.empty(size=[0,1000])\n",
    "    clean_label_holder = torch.empty(size=[0],dtype=torch.int8)\n",
    "    error_holder=torch.empty(size=[0,1000])\n",
    "    for cur_iter, (inputs, labels) in enumerate(train_loader):\n",
    "        holder_list.append([inputs,labels]) # testing only\n",
    "        print(f'Current iteration is {cur_iter}')\n",
    "        \n",
    "        # Transfer the data to the current GPU device.\n",
    "        if cfg.NUM_GPUS:\n",
    "            inputs = inputs.cuda(non_blocking=True)\n",
    "            labels = labels.cuda()\n",
    "        # lf - turning this off - stil need to one hot encode labels though\n",
    "        if cfg.MIXUP.ENABLE:\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "        # LF: adding this so that the labels are format [4,1,1000]. Existing form is just a tensor of dimension 4 whwen MIXUP.ENABLE is turned off\n",
    "        labels = torch.unsqueeze(convert_to_one_hot(labels,cfg.MODEL.NUM_CLASSES),dim=1) # use this one if the preds are shape [batch_size,49,1000]\n",
    "        #labels = convert_to_one_hot(labels,cfg.MODEL.NUM_CLASSES) # use this if the preds are shape [batch_size,1000]\n",
    "        holder_list2.append([inputs,labels]) # testing only\n",
    "        # Update the learning rate.\n",
    "        lr = optim.get_epoch_lr(cur_epoch + float(cur_iter) / data_size, cfg)\n",
    "        optim.set_lr(optimizer, lr)\n",
    "        #train_meter.data_toc()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=cfg.TRAIN.MIXED_PRECISION):\n",
    "            preds = model(inputs)\n",
    "            holder_list3.append([inputs,labels,preds])\n",
    "            ######################### ignore \n",
    "            ## LF: inserting own function to show predictions vs labels\n",
    "            # testpred_outcome_topk = torch.topk(preds,k=5,dim=1).indices\n",
    "            # testlabel_outcome = torch.argmax(labels,dim=1,keepdim=True)\n",
    "            # for i in range(0,len(testlabel_outcome)):\n",
    "            #     print('P:',testpred_outcome_topk[i], 'A:',testlabel_outcome[i])\n",
    "            ###########################\n",
    "            loss_fun = losses.get_loss_func(cfg.MODEL.LOSS_FUNC)(reduction=\"mean\")\n",
    "            # Compute the loss\n",
    "            loss = loss_fun(preds, labels)\n",
    "            \n",
    "        # check Nan Loss.\n",
    "        misc.check_nan_losses(loss)\n",
    "        # Perform the backward pass.\n",
    "        #print('running backward pass')\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Clip gradients if necessary\n",
    "        # if cfg.SOLVER.CLIP_GRAD_VAL:\n",
    "        #     torch.nn.utils.clip_grad_value_(\n",
    "        #         model.parameters(), cfg.SOLVER.CLIP_GRAD_VAL\n",
    "        #     )\n",
    "        # elif cfg.SOLVER.CLIP_GRAD_L2NORM:\n",
    "        #     torch.nn.utils.clip_grad_norm_(\n",
    "        #         model.parameters(), cfg.SOLVER.CLIP_GRAD_L2NORM\n",
    "        #     )\n",
    "        # Update the parameters.\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "#         if cfg.MIXUP.ENABLE:\n",
    "#             _top_max_k_vals, top_max_k_inds = torch.topk(\n",
    "#                 labels, 2, dim=1, largest=True, sorted=True\n",
    "#             )\n",
    "#             idx_top1 = torch.arange(labels.shape[0]), top_max_k_inds[:, 0]\n",
    "#             idx_top2 = torch.arange(labels.shape[0]), top_max_k_inds[:, 1]\n",
    "#             preds = preds.detach()\n",
    "#             preds[idx_top1] += preds[idx_top2]\n",
    "#             preds[idx_top2] = 0.0\n",
    "#             labels = top_max_k_inds[:, 0]\n",
    "\n",
    "        # LF: change format of preds and labels before passing into error functions\n",
    "        preds = torch.squeeze(torch.mean(preds,dim=1,keepdim=True),dim=1)\n",
    "        labels = torch.squeeze(torch.mean(labels,dim=1,keepdim=True),dim=1)\n",
    "        pred_holder = torch.cat((pred_holder,preds),dim=0)\n",
    "        label_holder = torch.cat((label_holder,labels),dim=0)\n",
    "        \n",
    "        #num_topks_correct = metrics.topks_correct(preds, labels, (1, 5))\n",
    "        num_topks_correct = topks_correct(pred_holder, label_holder, (1, 5))\n",
    "        top1_err, top5_err = [\n",
    "            (1.0 - x / pred_holder.size(0)) * 100.0 for x in num_topks_correct\n",
    "        ]\n",
    "        # Gather all the predictions across all the devices.\n",
    "        if cfg.NUM_GPUS > 1:\n",
    "            loss, top1_err, top5_err = du.all_reduce([loss, top1_err, top5_err])\n",
    "\n",
    "        # Copy the stats from GPU to CPU (sync point).\n",
    "        loss, top1_err, top5_err = (\n",
    "            loss.item(),\n",
    "            top1_err.item(),\n",
    "            top5_err.item(),\n",
    "        )\n",
    "        \n",
    "        clean_preds = torch.argmax(preds,dim=1)\n",
    "        clean_labels = torch.argmax(labels,dim=1)\n",
    "        clean_label_holder = torch.cat((clean_label_holder,clean_labels),dim=0)\n",
    "\n",
    "        # LF: calculate f1 score from torchmetrics - maybe we save this one for later \n",
    "        f1func = F1Score(num_classes=cfg.MODEL.NUM_CLASSES)\n",
    "        f1 = f1func(pred_holder, clean_label_holder)\n",
    "        # prediction printing lf\n",
    "        #print('Predicting...',clean_preds)\n",
    "        # label\n",
    "        #print('Labels...',clean_labels)\n",
    "        \n",
    "        # only print every X iterations\n",
    "        if cur_iter % 5 == 0:\n",
    "            print('Current top1_err:',top1_err,'Current top5_err:',top5_err)\n",
    "            print('Currentf1:',f1)\n",
    "            print('Loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47c977-ec46-414f-bc35-50dcce2144fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    \"\"\"\n",
    "    Train a model on train set and evaluate it on val set.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs. Details can be found in mvit/config/defaults.py\n",
    "    \"\"\"\n",
    "    # Set up environment.\n",
    "    du.init_distributed_training(cfg)\n",
    "    # Set random seed from configs.\n",
    "    np.random.seed(cfg.RNG_SEED)\n",
    "    torch.manual_seed(cfg.RNG_SEED)\n",
    "\n",
    "    # Setup logging format.\n",
    "    #logging.setup_logging(cfg.OUTPUT_DIR)\n",
    "\n",
    "    # Print config.\n",
    "    #logger.info(\"Train with config:\")\n",
    "    #logger.info(pprint.pformat(cfg))\n",
    "\n",
    "    # Build the model and print model statistics.\n",
    "    # removing this step because model is already built?\n",
    "    #model = build_model(cfg) # can either call this or just load the model not using the registry below.. note uing curr_model outputs a diferent shape output [1,49,1000] while this outputs [1,1000]\n",
    "    \n",
    "    # just loading the model not using registry\n",
    "    model = curr_model\n",
    "    #load_checkpoint(path_to_checkpoint=path_to_checkpoint, model=model) # don't need to reload the model\n",
    "    \n",
    "    #if du.is_master_proc() and cfg.LOG_MODEL_INFO:\n",
    "    #    misc.log_model_info(model, cfg, use_train_input=True)  # lf swiching this off because throwing irrelevant errors\n",
    "\n",
    "    # Construct the optimizer.\n",
    "    optimizer = optim.construct_optimizer(model, cfg)\n",
    "    # Create a GradScaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.TRAIN.MIXED_PRECISION)\n",
    "\n",
    "    # Load a checkpoint to resume training if applicable.\n",
    "    # start_epoch = cu.load_train_checkpoint(\n",
    "    #     cfg, model, optimizer, scaler if cfg.TRAIN.MIXED_PRECISION else None\n",
    "    # )\n",
    "\n",
    "    # Create the train and val loaders.\n",
    "    train_loader = loader.construct_loader(cfg, \"train\")\n",
    "    val_loader = loader.construct_loader(cfg, \"val\")\n",
    "\n",
    "    # Create meters.\n",
    "    train_meter = TrainMeter(len(train_loader), cfg)\n",
    "    val_meter = ValMeter(len(val_loader), cfg)\n",
    "\n",
    "    # Perform the training loop.\n",
    "    #logger.info(\"Start epoch: {}\".format(start_epoch + 1))\n",
    "    start_epoch = cfg.SOLVER.MAX_EPOCH - 2 # manually set start epoch\n",
    "    epoch_timer = EpochTimer()\n",
    "    print('Going from start_epoch: ',start_epoch, 'to..', cfg.SOLVER.MAX_EPOCH)\n",
    "    for cur_epoch in range(start_epoch, cfg.SOLVER.MAX_EPOCH):\n",
    "        # Shuffle the dataset.\n",
    "        loader.shuffle_dataset(train_loader, cur_epoch)\n",
    "\n",
    "        # Train for one epoch.\n",
    "        epoch_timer.epoch_tic()\n",
    "        train_epoch(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scaler,\n",
    "            train_meter,\n",
    "            cur_epoch,\n",
    "            cfg,\n",
    "        )\n",
    "        epoch_timer.epoch_toc()\n",
    "        logger.info(\n",
    "           f\"Epoch {cur_epoch} takes {epoch_timer.last_epoch_time():.2f}s. Epochs \"\n",
    "           f\"from {start_epoch} to {cur_epoch} take \"\n",
    "           f\"{epoch_timer.avg_epoch_time():.2f}s in average and \"\n",
    "           f\"{epoch_timer.median_epoch_time():.2f}s in median.\"\n",
    "        )\n",
    "        logger.info(\n",
    "           f\"For epoch {cur_epoch}, each iteraction takes \"\n",
    "           f\"{epoch_timer.last_epoch_time()/len(train_loader):.2f}s in average. \"\n",
    "           f\"From epoch {start_epoch} to {cur_epoch}, each iteraction takes \"\n",
    "           f\"{epoch_timer.avg_epoch_time()/len(train_loader):.2f}s in average.\"\n",
    "        )\n",
    "\n",
    "        is_checkp_epoch = cu.is_checkpoint_epoch(\n",
    "            cfg,\n",
    "            cur_epoch,\n",
    "        )\n",
    "        is_eval_epoch = misc.is_eval_epoch(cfg, cur_epoch)\n",
    "\n",
    "        # Save a checkpoint.\n",
    "        if is_checkp_epoch:\n",
    "            cu.save_checkpoint(\n",
    "                cfg.OUTPUT_DIR,\n",
    "                model,\n",
    "                optimizer,\n",
    "                cur_epoch,\n",
    "                cfg,\n",
    "                scaler if cfg.TRAIN.MIXED_PRECISION else None,\n",
    "            )\n",
    "        # Evaluate the model on validation set.\n",
    "        if is_eval_epoch:\n",
    "            eval_epoch(val_loader, model, val_meter, cur_epoch, cfg)\n",
    "            \n",
    "@torch.no_grad()\n",
    "def eval_epoch(val_loader, model, val_meter, cur_epoch, cfg):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the val set.\n",
    "    Args:\n",
    "        val_loader (loader): data loader to provide validation data.\n",
    "        model (model): model to evaluate the performance.\n",
    "        val_meter (ValMeter): meter instance to record and calculate the metrics.\n",
    "        cur_epoch (int): number of the current epoch of training.\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            mvit/config/defaults.py\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluation mode enabled. The running stats would not be updated.\n",
    "    model.eval()\n",
    "    val_meter.iter_tic()\n",
    "\n",
    "    for cur_iter, (inputs, labels) in enumerate(val_loader):\n",
    "        if cfg.NUM_GPUS:\n",
    "            # Transferthe data to the current GPU device.\n",
    "            inputs = inputs.cuda(non_blocking=True)\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        val_meter.data_toc()\n",
    "\n",
    "        preds = model(inputs)\n",
    "\n",
    "        # select first 1000 IN1K classes for evaluation for IN21k\n",
    "        if cfg.DATA.IN22k_VAL_IN1K != \"\":\n",
    "            preds = preds[:, :1000]\n",
    "\n",
    "        # Compute the errors.\n",
    "        num_topks_correct = metrics.topks_correct(preds, labels, (1, 5))\n",
    "\n",
    "        # Combine the errors across the GPUs.\n",
    "        top1_err, top5_err = [\n",
    "            (1.0 - x / preds.size(0)) * 100.0 for x in num_topks_correct\n",
    "        ]\n",
    "        if cfg.NUM_GPUS > 1:\n",
    "            top1_err, top5_err = du.all_reduce([top1_err, top5_err])\n",
    "\n",
    "        # Copy the errors from GPU to CPU (sync point).\n",
    "        top1_err, top5_err = top1_err.item(), top5_err.item()\n",
    "\n",
    "        val_meter.iter_toc()\n",
    "        # Update and log stats.\n",
    "        val_meter.update_stats(\n",
    "            top1_err,\n",
    "            top5_err,\n",
    "            inputs[0].size(0)\n",
    "            * max(\n",
    "                cfg.NUM_GPUS, 1\n",
    "            ),  # If running  on CPU (cfg.NUM_GPUS == 1), use 1 to represent 1 CPU.\n",
    "        )\n",
    "        val_meter.update_predictions(preds, labels)\n",
    "        val_meter.log_iter_stats(cur_epoch, cur_iter)\n",
    "        val_meter.iter_tic()\n",
    "\n",
    "    # Log epoch stats.\n",
    "    val_meter.log_epoch_stats(cur_epoch)\n",
    "    val_meter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a07c94-d324-4121-aee5-31b6b0780885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set epoch to train to\n",
    "cfg.SOLVER.MAX_EPOCH = 301\n",
    "# seems like certain epochs are evaluation epochs?\n",
    "cfg.TRAIN.AUTO_RESUME = False\n",
    "#cfg.TRAIN.CHECKPOINT_FILE_PATH = \"MViTv2_T_in1k.pyth\" # this is needed for load train checkpoint to work\n",
    "cfg.SOLVER.OPTIMIZING_METHOD = \"adamw\" # this is the default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47200bb-1117-4341-a558-0fd85ec7a1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn 0, non bn 95, zero 147 no grad 0\n",
      "Going from start_epoch:  299 to.. 301\n"
     ]
    }
   ],
   "source": [
    "# seems like loading the pre-trained model leads to issues?\n",
    "# what if we try\n",
    "# if you turn off scaler.step(optimizer) it seems to work\n",
    "train(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effba639-c3f5-47a5-94c8-e609d0db5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output model object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('hyka_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "86e628cd01d07b536d87d514a6735c69b116d2f1348eeb31dc1baa640a8ef7aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
